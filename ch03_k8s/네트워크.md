Kubernetes 네트워크는 크게 4가지로 분류된다.

* 서로 결합된 컨테이너와 컨테이너 간 통신
* Pod와 Pod 간의 통신
* Pod와 Service간의 통신
* 외부와 Service간의 통신

## Container Network Interfaces(CNI)
기본적으로 kubernetes는 kubenet이라는 아주 기본적이고 간단한 네트워크 플러그인을 제공 해주지만,  
정말로 매우 기본적이고 간단한 기능만 제공해주는 네트워크 플러그인이기 때문에 이 자체로는 크로스 노드 네트워킹이나 네트워크 정책 설정과 같은 고급 기능은 구현되어 있지 않다.  

따라서 kubernetes에서는 Pod 네트워킹 인터페이스로 CNI 스펙을 준수하는 다양한 네트워크 플러그인을 사용하는 것을 권장한다.  


## 컨테이너와 컨테이너 간 통신
![image](https://user-images.githubusercontent.com/67637716/175893278-beaafd49-9c8c-4c8f-a62e-c65d0ab58b41.png)  
Docker에서는 기본적으로 같은 노드(host) 내의 컨테이너 끼리의 통신은 위 그림과 같이 docker0라는 가상 네트워크 인터페이스(172.17.0.0/24)를 통해 가능하다.  
또한 각 컨테이너는 veth(Virtual Ethernet)라는 가상 네트워크 인터페이스를 고유하게 가지며 따라서 각각의 veth IP 주소 값으로 통신할 수 있다.  

![image](https://user-images.githubusercontent.com/67637716/175893430-40f63ec1-8d9e-4ee1-844c-13729a2ee395.png)  
위 그림은 Kubernetes Pod내 컨테이너끼리의 통신이다. (Docker도 가능)  
쿠버네티스는 도커와는 달리 파드단위로 컨테이너들을 관리한다.  
파드는 여러개의 컨테이너로 구성 될 수 있는데 컨테이너들은 모두 동일한 IP를 부여받을 수 있다.  
이 때 동일한 IP를 부여받게끔 해주는 것이 바로 pause 라는 컨테이너다.  

veth0 가상 네트워크 인터페이스에 두 개의 컨테이너가 동시에 할당되어 있다.  
즉, 두 개의 컨테이너는 모두 veth0 라는 동일한 네트워크를 사용하는 것이다.  

#### veth0 안에서 각 컨테이너는 고유한 port 번호로 서로를 구분한다.  


## Pod와 Pod 간의 통신
* 싱글 노드 Pod 네트워크  
![image](https://user-images.githubusercontent.com/67637716/175895603-80bc2b16-7106-4caa-badc-2e552b254c05.png)  
Pod의 특징 중 하나로 각 Pod는 고유한 IP 주소를 가진다. (이것이 위에서 설명한 가상 네트워크 인터페이스 veth이다.)
 Pod는 kubenet 혹은 CNI 로 구성된 네트워크 인터페이스를 통하여 고유한 IP 주소로 서로 통신할 수 있다.  
 
 * 멀티 노드 Pod 네트워크  
 ![image](https://user-images.githubusercontent.com/67637716/175896209-64fcc0e0-574c-4409-a436-8996060848ae.png)  
 여러 개의 워커 노드 사이에 각각 다른 노드에 존재하는 Pod가 서로 통신하려면 라우터를 거쳐서 통신하게 된다.  
 
 
 ## Pod와 Service간의 통신
 Pod는 기본적으로 쉽게 대체될 수 있는 존재이기 때문에 Pod to Pod Network만으로는 Kubernetes 시스템을 내구성있게 구축할 수 없다.  
어떠한 말이냐면, Pod IP를 어떤 서비스의 엔드포인트로 설정하는 것은 가능하지만,  
해당 Pod가 계속 존재하고 있을 것이라는 보장도 없고 새로운 Pod가 생성되었을 때 그 IP 주소가 엔드포인트와 동일할 것이라고 보장할 수 없다는 것이다.  

이를 해결하기 위해서는 서비스 앞단에 reverse-proxy(혹은 Load Balancer)를 위치시키는 방법이 있다.  

※ reverse-proxy : https://losskatsu.github.io/it-infra/reverse-proxy/#  

클라이언트에서 proxy로 연결을 하면 proxy의 역할은 서버들 목록을 관리하며 현재 살아있는 서버에게 트래픽을 전달하는 것이다.  

#### service란 각 Pod로 트래픽을 포워딩 해주는 프록시 역할을 한다.  
이 때 selector 라는 것을 이용하여 트래픽을 전달받을 Pod들을 결정  
Pod 네트워크와 동일하게 service 네트워크 또한 가상 IP 주소이다.  

하지만 Pod 네트워크와는 조금 다르게 동작한다.  
Pod 네트워크는 실질적으로 가상 이더넷 네트워크 인터페이스(veth)가 세팅되어져 ifconfig에서 조회할 수 있지만, service 네트워크는 ifconfig로 조회할 수 없다.  
또한 routing 테이블에서도 service 네트워크에 대한 경로를 찾아볼 수 없다.  

### service 네트워크의 동작 방식
![image](https://user-images.githubusercontent.com/67637716/175899851-3d2b8ddb-d233-4769-b3b4-efafa5e3441f.png)  
두 개의 워커 노드가 있고 하나의 게이트웨이를 통해 서로 연결되어있다.  
또한 마스터 노드에서(이미지에는 보이지 않지만) 아래의 명세서를 통해 service네트워크를 생성했다고 가정한다.  
``` ruby
apiVersion: v1
kind: Service
metadata:
  name: service-test # service의 이름
spec:
  selector:
    app: server_pod1 # 10.0.1.2와 10.0.2.2에서 돌아가고 있는 서버 컨테이너의 pod 라벨
  ports:
    - protocol: TCP
      port: 80 # service에서 서버 컨테이너 어플리케이션과 매핑시킬 포트 번호
      targetPort: 8080 # 서버 컨테이너에서 구동되고 있는 서버 어플리케이션 포트 번호
```  

client pod가 service 네트워크를 통해 server pod1으로 http request를 요청하는 과정은 아래와 같다.  
![image](https://user-images.githubusercontent.com/67637716/175900314-d1fd1340-516f-490d-967e-860b02d2b2dd.png)  

* client pod가 http request를 service-test라는 DNS 이름으로 요청한다.
* 클러스터 DNS 서버(coredns)가 해당 이름을 service IP(예시로 10.3.241.152 이라고 한다)로 매핑시켜준다.
* http 클라이언트는 DNS로부터 IP를 이용하여 최종적으로 요청을 보내게 된다.

IP 네트워크(Layer 3)는 기본적으로 자신의 host에서 목적지를 찾지 못하면 상위 게이트웨이로 패킷을 전달하도록 동작한다.  
client pod 안에 들어 있는 첫번째 가상 이더넷 인터페이스(veth1)에서 목적지 IP를 보게 되고  
10.3.241.152라는 주소에 대해 전혀 알지 못하기 때문에 다음 게이트웨이(cbr0)로 패킷을 넘기게 된다.  
cbr0는 bridge이기 때문에 단순히 다음 게이트웨이(eth0)로 패킷을 전달한다.  
여기서도 마찬가지로 eth0라는 이더넷 인터페이스가 10.3.241.152라는 IP 주소에 대해서 모르기 때문에 보통이라면 최상위에 존재하는 게이트웨이로 전달될 것이다.  

하지만 예상과는 달리 갑자기 패킷의 목적지 주소가 변경되어 server pod1 중 하나로 패킷이 전달되게 된다.  

<br>
패킷의 흐름이 이렇게 될 수 있는 이유는 `kube-proxy` 라는 컴포넌트 때문이다.  

쿠버네티스는 리눅스 커널 기능 중 하나인 netfilter와 user space에 존재하는 인터페이스인 iptables라는 소프트웨어를 이용하여 패킷 흐름을 제어한다.  

netfilter란 Rule-based 패킷 처리 엔진이며, kernel space에 위치하여 모든 오고 가는 패킷의 생명주기를 관찰한다.  
그리고 규칙에 매칭되는 패킷을 발견하면 미리 정의된 action을 수행한다.  
iptables는 netfilter를 이용하여 chain rule이라는 규칙을 지정하여 패킷을 포워딩 하도록 네트워크를 설정한다.  

... ~ 더 파고 들면 너무 어렵네요

결론 : 이런 방식은 클러스터 내부의 Pod에서 요청한 request에 한해서만 위와 같은 방식으로 동작한다.  
즉, 외부에서 들어온 요청에 대해서는 다르게 처리해야 한다.  

## 외부와 Service간의 통신
정리!  
* service:
  * Pod로 액세스 할 수 있는 정책을 정의하는 추상화된 개념  
  * Pod로 트래픽을 포워딩 해주는 프록시 역할
* selector: 트래픽을 전달받을 Pod들을 결정
* service 네트워크: Service가 할당받는 네트워크 인터페이스
* Service는 기본적으로 Cluster-IP라는 IP 주소를 부여받으며, 클러스터 내부적으로 이 IP 주소를 통해 자신이 포워딩 해야 할 Pod들에게 트래픽을 전달  
* Service는 클러스터 내부적으로만 통신


#### Service에서 외부 통신을 가능하게 해주는 Service의 타입
* NodePort
* Load Balancer

#### NodePort
![image](https://user-images.githubusercontent.com/67637716/175903743-2c3fd814-b1be-4b87-8584-301026dc5acd.png)  
NodePort 타입 서비스는 노드 네트워크의 IP를 통하여 접근 할 수 있을 뿐만 아니라 ClusterIP로도 접근이 가능하다.  
쿠버네티스가 NodePort 타입의 서비스를 생성하면 kube-proxy가 각 노드의 eth0 네트워크 interface에 30000–32767 포트 사이의 임의의 포트를 할당한다.  
그리고 할당된 포트로 요청이 오게 되면 이것을 매핑된 ClusterIP로 전달한다.  
 
#### Load Balancer
AWS나 GCP같은 외부 클라우드 서비스를 사용하여 로드밸런서를 프로비저닝 할 수 있는 경우에 사용할 수 있는 Service 타입  
외부 로드밸런서의 트래픽은 클러스터 내의 Pod로 전달된다.  




