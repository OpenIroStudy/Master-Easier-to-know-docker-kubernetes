# 스케줄링
쿠버네티스는 멀티 노드로 운영되며, 여러 대의 노드를 한 대의 인스턴스를 사용하는 것과 같은 효과를 준다. <br>
쿠버네티스에서는 노드의 상황에 따라 어떤 방식에 의해 파드를 배치하는 것을 스케줄링이라고 한다. <br>

<br>  
![image](https://user-images.githubusercontent.com/43237961/179392168-f9bc87b4-1998-4c04-9905-4c09f731e3dd.png)

<br>  
위 그림은 사용자가 파드를 생성하는 명령을 실행하면 어떤 식으로 각 요소가 커뮤니케이션을 하면서 <br> 
해당 파드가 노드에 스케줄링 되는지 보여준다. <br> 
<br><br> 

1. 사용자가 쿠버네티스에 파드 생성을 요청한다.
2. 쿠버네티스가 받은 모든 요청은 API Server가 다루기 때문에 이 명령 또함 API Server에 전달된다. 
3. API Server는 모든 요소의 데이터 저장소인 etcd에 정보를 저장하고 사용자에게 결과를 전달한다. 
4. API Server는 파드 배치에 권한이 있는 스케줄러에게 해당 파드를 생성하라는 정보를 전달하고 스케줄러는 파드에 대한 정보를 인지한다.
5. 스케줄러는 노드들 중 파드를 배치하기 적합한 노드를 내부적인 룰에 따라 결정하고 API Server에게 통보한다. 
6. API Server는 스케줄러에게 통보받은 노드에 Kubelet을 통하여 파드를 생성하는 명령을 실행한다.
7. Kubelet은 파드를 생성하는 명령을 실행시키면서 파드에 도커 컨테이너를 배치한다. 


--> 대략적으로 스케줄링을 이렇게 한다. <br> 

좀 더 쉬운 설명 <br>

kubectl apply 등의 명령어를 통해 사용자의 Pod의 생성 요청이 kube-apiserver에 제출되면 <br> 
apiserver는 etcd에 Pod의 정보를 저장합니다. <br> 
단, pod가 위치한 노드의 정보인 NodeName의 값이 설정되지 않은 상태로 Pod의 정보를 저장하죠.<br> 
이 때, kube-scheduler는 이러한 정보를 watch하고 있다가 Pod의 NodeName이 비어 있는 상태라는 것을 감지합니다.<br> 
1 따라서 kube-scheduler는 해당 Pod를 할당하기에 적절한 노드를 찾는, 일종의 스케줄링 작업을 진행합니다. <br> 
적절한 노드를 찾았다면, kube-scheduler는 "야, 이 Pod는 이 노드가 적당하다" 라는 메시지를 kube-apiserver에게 전달합니다. <br> 
즉, kube-scheduler 자체가 Pod를 직접 생성하는 것은 아니며, 최종적으로는 할당 노드의 kubelet에서 Pod를 생성하는 셈입니다. <br> 
<br><br> 



- 노드 필터링
- 노드 우선 순위 계산
- 실제 예약 작업
- --> 세 단계를 거쳐 새로운 파드를 어떤 노드에 배치할 것인지 결정한다. 
<br><br> 

# 필터링이란?
노드 필터링 단계는 Pod에 설정된 제한 조건 (Constraint) 및 자원 요구량 (request) 등을 통해 적절한 노드를 걸러냅니다. <br>
<br>
사실상 개발자가 가장 많이 사용할 법한 방법은 첫 번째인 노드 필터링 단계입니다. <br>
Pod에 요구하는 리소스만큼의 가용 자원이 존재하는 노드를 선택해야 한다거나, <br>
특정 Pod와 다른 AZ의 노드에 할당함으로써 고가용성을 보장해야 한다거나, <br>
특정 Pod와 반드시 같은 노드에 위치시킴으로써 네트워크 지연시간을 최대한 줄이는 등의 노력이다.<br><br>

* 포트 필터 
  * 파드가 요구하는 포트를 노드에서 사용할 수 있는지 확인한다. 
* 호스트 네임 필터
  * 파드를 생성할 때 특정한 호스트 네임을 지정할 수 있다. 
* 리소스 필터
  * 해당 노드에 파드를 배포할 만한 충분한 리소스 (CPU, Memory, Disk)가 있는지 확인 
* 노드 셀렉터 필터
  * 파드를 생성할 떄 노드 셀렉터를 지정할 수 있다. 
* 볼륨 필터
  * 파드를 생성할 떄, 이 파드가 요구하는 디스크에 대한 부분을 수용할 수 있는지 확인한다. 
  * 예를 들어 파드가 클라우드의 특정 zone에 대한 디스크를 볼륨으로 요구할 떄, 
  * 이 부분을 수정할 수 있는 노드만 해당 파드에 배치된다.   
<br><br> 

## Taints Toleration
사전적인 정의 '얼룩', '용인하기' <br>
쿠버네티스의 노드에는 Taints를 설정함으로써 해당 노드에 Pod가 할당 또는 실행되는 것을 막을 수 있습니다. <br> 
Pod의 정의에 Toleration을 설정하면 Taints가 있는 노드에도 할당하는 것이 가능합니다. <br> 
노드에 taints(얼룩)이 졌지만, 이 얼룩을 toleration(용인)할 수 있는 Pod만이 그 노드에 할당될 수 있는 것입니다. <br> 
 가장 간단한 Taints는 쿠버네티스의 마스터 노드에 설정된 Taints입니다. <br> 
 
![image](https://user-images.githubusercontent.com/43237961/179393084-d625087e-cd52-4955-a297-6e3570d572c4.png)
<br> 
마스터 노드에는 기본적으로 Pod가 할당되지 않는다. 할당되지 않는 것은 마스터 노드에는 기본적으로 Taints가 설정되어 있기 때문이다. <br> 
node-role.kubernetes.io/master 라는 이름으로 NoScheule 이라는 값이 들어 있다. <br> 
NoSchedule 이라는 설정 덕분에 일반적인 Pod들은 마스터 노드에 할당되지 않는 것이다. <br> 
이에 대한 Toleration을 설정하면 마스터 노드에서도 Pod를 할당하는 것이 가능합니다. <br> 
![image](https://user-images.githubusercontent.com/43237961/179393229-c128a2df-f95f-4f8e-935b-bb377302dbb5.png)
<br> 
tolerations 항목에 node-role.kubernetes.io/master 라는 키로 설정된 NoSchedule 효과를 용인 (Toleration) 하겠다는 설정이 되어 있습니다. <br>
위와 같은 설정을 하면 마스터 노드의 Taints를 견뎌낼 수 있기 때문에, Pod를 스케줄링 할 때 마스터 노드 또한 할당 가능한 노드로서 선정됩니다. <br> 

### Taints와 Toleration 기본 사용 방법

이러한 Taints는 아래의 명령어를 통해 노드에 설정하고 해제할 수 있습니다. <br> 
아래 예시는 key=value라는 키와 값으로 NoSchedule를 설정한 뒤, key라는 키의 NoSchedule Taints를 해제합니다. <br>
```
$ kubectl taint nodes <NODE_NAME> key=value:NoSchedule
 
$ kubectl taint nodes <NODE_NAME> key:NoSchedule-
```

이렇게 직접 설정된 Taints에 대한 Toleration은 아래와 같이 Pod에 설정할 수 있습니다. <br>
operator는 Equal 외에도 특정 키가 존재하는 경우에는 모두 적용하는 Exist를 사용할 수도 있습니다.<br>
![image](https://user-images.githubusercontent.com/43237961/179393346-2194fa82-e08e-4eea-a04c-15fd75048888.png)
<br><br>
위 예시는 key와 value 값, 그리고 effect가 일치하는 Taints에 대해서 적용될 것이라는 뜻입니다. <br> 
effect에는 NoSchedule 외에도 NoExecute와 PreferNoSchedule를 사용할 수 있습니다. <br> 
PreferNoSchedule은 NoSchedule과 동일한 효과를 내지만, '가능하면 이 노드에 할당하지 말아라' <br>
라는 의미의 Soft 제한이기 때문에, 여타 이유로 인해 결국은 Pod가 할당될 수도 있습니다. <br> 

<br>
그러나 NoExecute는 NoSchedule과 다른 기능입니다. <br> 
이미 Pod가 할당된 노드에 대해 NoSchedule을 설정하더라도 실행 중인 Pod는 어쨌거나 계속 실행되지만, <br> 
NoExecute를 설정할 경우 기존에 실행 중인 Pod가 종료된 뒤 다른 노드로 쫓겨납니다 (이를 evict라고 표현합니다). <br> 
즉, 실행조차 허용하지 않을 것이냐, 스케줄링 단계에서만 허용하지 않을것이냐, 가 주된 차이점입니다. <br> 
<br>

이러한 원리를 잘 이용하면 다양한 Pod 할당 방법을 사용할 수 있습니다. <br> 
예를 들어, GPU가 존재하는 노드에 Taints를 설정한 뒤, cuda/tensorflow같은 Pod들은 해당 Taints에 해당하는 Toleration을 정의해 생성하는 방식 등을 생각해 볼 수 있습니다. <br>
<br>  

### 자동으로 설정되는 Taints와 Toleration 

위에서 언급했던 마스터 노드의 Taints처럼, Pod 또한 기본적으로 설정되어 있는 Toleration이 있습니다. <br>
별도로 Toleration을 설정하지 않았더라도, 쿠버네티스는 자동으로 2개의 Toleration을 설정합니다. <br>
![image](https://user-images.githubusercontent.com/43237961/179393437-0f24f4a9-9f8c-473a-a938-9eb3e99bb224.png)

<br><br> 

not-ready:NoExecute와 unreachable:NoExecute의 Toleration이 자동으로 설정되어 있는 것을 알 수 있습니다. <br>
이에 대응하는 Taints는 Worker 노드가 바람직하지 않은 상태임이 감지되었을 때 쿠버네티스에 의해 자동으로 설정됩니다. <br> 
즉, 통신이 불가능하거나 에러가 발생한 노드들은 not-ready, unreachable 상태가 될 것이고, 자동으로 NoExecute Taints가 설정되는 것입니다. <br> 
<br> 
그러나 쿠버네티스를 운영해보셨다면 아시겠지만, 노드에 장애가 생겨도 즉시 Pod가 다른 노드로 옮겨가지는 않습니다.<br>
근데? 안 운영해봄 ㅜ <br>
이는 위의 두 Toleration에 설정된 tolerationSeconds 때문입니다. <br> 
이 값을 직접 눈으로 보기 위해 kubectl edit으로 Pod의 전체 정보를 확인해 보겠습니다. <br>

![image](https://user-images.githubusercontent.com/43237961/179393543-363d3184-68cf-4623-a22e-a605f636ce9f.png)
<br><br> 
그러시구나~ <br> 
tolerationSeconds가 300으로 설정되어 있으며, 이는 Taints가 추가된 뒤 300초 뒤에 다른 노드로 쫓겨날 것을 의미합니다.<br> 
노드에 장애가 발생해 Taints가 해당 노드에 추가된다면 300초 뒤에 다른 노드로 Pod들이 옮겨가게 됩니다. <br> 

## 특정 노드 선택 및 Affinity
## NodeName과 NodeSelector 

Pod의 Spec에 nodeName을 설정하면 특정 노드에 Pod를 할당할 수 있습니다.  <br> 
![image](https://user-images.githubusercontent.com/43237961/179393608-7fe3afe0-b76b-4793-8a42-9609bf7319f8.png)
<br> 
사실 제일 간단하고 원시적인 (?) 방법입니다. <br> 
그러나 이렇게 YAML을 작성하면 보편적이지 않기 때문에 그다지 바람직해 보이지는 않습니다. <br>
nodeName 대신, 노드에 라벨을 설정한 뒤 특정 라벨을 가지는 노드에만 Pod를 생성하도록 스케줄링하는 방법이 좀 더 배포하기에는 좋을 것 같습니다. <br> 
노드 라벨은 아래와 같은 명령어로 설정할 수 있습니다. <br> 
```
kubectl label nodes <NODE_NAME> disktype=ssd
```

설정된 라벨은 kubectl get node --show-labels 명령어로 확인할 수 있습니다. <br> 
라벨은 직접 설정할 수도 있지만, 쿠버네티스에 의해서 자동으로 설정되는 라벨들도 있습니다. <br> 
아래의 예시에서 알 수 있듯이, 노드의 호스트 이름, 사용 Region과 AZ 등도 라벨로서 자동적으로 추가됩니다. <br> 
![image](https://user-images.githubusercontent.com/43237961/179393683-47e6cd6c-8e43-4955-aec4-60ae2abcf121.png)
 <br> 
 Pod의 정의에 nodeSelector를 추가한 뒤 라벨을 설정합니다. <br> 
 이 라벨을 가지는 노드에만 Pod가 할당되도록 스케줄링됩니다. <br>
![image](https://user-images.githubusercontent.com/43237961/179393869-7e62820f-9c53-4aba-bb40-864417c3b8a8.png)
<br> 
nodeName 대신 nodeSelector를 사용함으로써,  <br> 
적어도 노드 이름에 종속적이지는 않도록 YAML을 작성할 수 있습니다. <br>

## Node Affinity 

위와 같이 nodeSelector를 사용해도 원하는 노드에 Pod를 배치할 수 있겠지만, <br> 
라벨링 기반의 스케줄링은 단순히 키 값이 같은지만을 비교해 노드를 할당하기 때문에 활용 방법이 비교적 제한되어 있습니다. <br>
이를 보완하는 방법이 Node Affinity 방법입니다. <br> 
Node Affinity는 제한적으로나마 프로그래밍 방식의 접근이 가능하기도 하고, <br> 
Soft / Hard 방식의 노드 선택 또한 가능합니다. <br> 

<br><br> 

다음 두 가지 방식으로 Node Affinity를 사용할 수 있습니다. <br> 
개인적인 생각이지만, Affinity 종류의 선언자는 이름이 (짜증나게) 길어서, 쓰기 귀찮을 때도 있습니다. <br> 
그러시구나,,,,, <br> 
- requiredDuringSchedulingIgnoredDuringExecution : 반드시 특정 라벨을 가지는 노드에만 할당되도록 합니다. <br>
- preferredDuringSchedulingIgnoredDuringExecution : 가능하면 특정 라벨을 가지는 노드에 할당되도록 노력합니다. <br> 

* requiredDuringSchedulingIgnoredDuringExecution

nodeSelector와 다른 점은 Soft / Hard 제한을 설정할 수 있다는 점 뿐인것 같지만, <br> 
nodeSelector보다 좀 더 넓은 방식으로 사용할 수 있습니다. <br> 
예시를 통해 살펴보도록 하겠습니다.<br>
![image](https://user-images.githubusercontent.com/43237961/179394007-7016ab99-f5fa-41c3-b8ba-a44709caa6d8.png)

<br> 
위는 nodeAffinity를 사용하는 단순한 예시입니다.<br>
얼핏 보면 매우 복잡해 보이지만, 빨간 박스의 부분만을 이해하면 됩니다. <br> 
Worker 노드 중, 키가 kubernetes.io/e2e-az-name 이고 값이 e2e-az1, e2e-az2 인 라벨을 가지는 노드에 Pod를 할당하라는 의미입니다. <br>
operator가 In으로 설정되어 있기 때문에 e2e-az1 또는 e2e-az2 중 하나의 값을 가지는 노드를 선택합니다.<br> 
operator에는 In 외에도 NotIn, Exists, DoesNotExist, Gt, Lt 값을 사용할 수 있기 때문에, nodeSelector보다 더욱 넓은 활용이 가능하다는 것이 특징입니다.<br> 
